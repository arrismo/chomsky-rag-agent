{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df9b019e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmoise/Documents/Learning/chomsky-rag-agent/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "import bs4\n",
    "from typing import List\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.agents import create_agent\n",
    "from langchain.tools import tool\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ecbd67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = os.getenv(\"LM_MODEL\")\n",
    "base_url = os.getenv(\"OPENAI_BASE_URL\")\n",
    "\n",
    "model = init_chat_model(\n",
    "    model_name,\n",
    "    model_provider=\"openai\",\n",
    "    base_url=base_url,\n",
    "    api_key=\"lm-studio\"\n",
    "    )\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chomksy-db\", \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8d30b3",
   "metadata": {},
   "source": [
    "Load, Split, Embed, Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b5731b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 328 articles\n",
      "Loaded 328 documents, total characters: 14497624\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "ARTICLES_INDEX_URL = \"https://chomsky.info/articles/\"\n",
    "\n",
    "resp = requests.get(ARTICLES_INDEX_URL)\n",
    "resp.raise_for_status()\n",
    "soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "\n",
    "# Match full dates like \"May 12 2023\" or \"March 27, 2023\"\n",
    "date_regex = re.compile(\n",
    "    r\"\\b(\"\n",
    "    r\"January|February|March|April|May|June|July|August|September|October|November|December\"\n",
    "    r\")\\s+\\d{1,2},?\\s+\\d{4}\\b\"\n",
    ")\n",
    "\n",
    "articles = []\n",
    "for li in soup.select(\"#main_container > ul > li\"):\n",
    "    a = li.find(\"a\")\n",
    "    if not a:\n",
    "        continue\n",
    "    title = a.get_text(strip=True)\n",
    "    href = urljoin(ARTICLES_INDEX_URL, a.get(\"href\", \"\").strip())\n",
    "\n",
    "    # Full text of the <li>\n",
    "    li_text = li.get_text(\" \", strip=True)\n",
    "\n",
    "    # Try to find the full date in the text\n",
    "    m = date_regex.search(li_text)\n",
    "    raw_date = m.group(0) if m else None\n",
    "\n",
    "    # (Optional) normalize to ISO \"YYYY-MM-DD\"\n",
    "    iso_date = None\n",
    "    if raw_date:\n",
    "        cleaned = raw_date.replace(\",\", \"\")\n",
    "        try:\n",
    "            iso_date = datetime.strptime(cleaned, \"%B %d %Y\").date().isoformat()\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    articles.append(\n",
    "        {\n",
    "            \"title\": title,\n",
    "            \"url\": href,\n",
    "            \"raw_date\": raw_date,\n",
    "            \"date\": iso_date or raw_date,  # keep something useful in \"date\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(f\"Found {len(articles)} articles\")\n",
    "\n",
    "# 3. Load full article pages\n",
    "article_urls = [a[\"url\"] for a in articles]\n",
    "loader = WebBaseLoader(article_urls)\n",
    "docs = loader.load()\n",
    "\n",
    "# 4. Attach title and date (and any other metadata) to each Document\n",
    "title_by_url = {a[\"url\"]: a[\"title\"] for a in articles}\n",
    "date_by_url = {a[\"url\"]: a.get(\"date\") for a in articles}\n",
    "\n",
    "for doc in docs:\n",
    "    url = doc.metadata.get(\"source\") or doc.metadata.get(\"url\")\n",
    "    if url in title_by_url:\n",
    "        doc.metadata[\"title\"] = title_by_url[url]\n",
    "    if url in date_by_url and date_by_url[url]:\n",
    "        doc.metadata[\"date\"] = date_by_url[url]\n",
    "    # You can also tag type, etc.\n",
    "    doc.metadata.setdefault(\"type\", \"article\")\n",
    "\n",
    "total_chars = sum(len(doc.page_content) for doc in docs)\n",
    "print(f\"Loaded {len(docs)} documents, total characters: {total_chars}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b23d606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 21595 chunks\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# chunk_overlap specifies the number of characters of overlap between consecutive text chunks,\n",
    "# which helps preserve context between splits.\n",
    "# add_start_index, when set to True, includes the start index of the chunk in the split metadata.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200,     \n",
    "    add_start_index=True  \n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "print(f\"Split into {len(all_splits)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ece2006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('chomsky_chunks.jsonl')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "output_path = Path(\"chomsky_chunks.jsonl\")\n",
    "\n",
    "with output_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for i, doc in enumerate(all_splits):\n",
    "        record = {\n",
    "            \"id\": i,\n",
    "            \"url\": doc.metadata.get(\"source\") or doc.metadata.get(\"url\"),\n",
    "            \"article_title\": doc.metadata.get(\"title\"),\n",
    "            \"article_date\": doc.metadata.get(\"date\"),\n",
    "            \"article_source\": doc.metadata.get(\"source\"),\n",
    "            \"chunk_index\": doc.metadata.get(\"start_index\"),\n",
    "            \"content\": doc.page_content,\n",
    "            \"metadata\": doc.metadata,\n",
    "        }\n",
    "        f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1672cc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added batch 1/5 (5000 documents)\n",
      "Added batch 2/5 (5000 documents)\n",
      "Added batch 3/5 (5000 documents)\n",
      "Added batch 4/5 (5000 documents)\n",
      "Added batch 5/5 (1595 documents)\n",
      "\n",
      "Total documents added: 21595\n"
     ]
    }
   ],
   "source": [
    "# Add documents in batches to avoid exceeding Chroma's max batch size\n",
    "batch_size = 5000  \n",
    "document_ids = []\n",
    "\n",
    "for i in range(0, len(all_splits), batch_size):\n",
    "    batch = all_splits[i:i + batch_size]\n",
    "    batch_ids = vector_store.add_documents(batch)\n",
    "    document_ids.extend(batch_ids)\n",
    "    print(f\"Added batch {i//batch_size + 1}/{(len(all_splits) + batch_size - 1)//batch_size} ({len(batch)} documents)\")\n",
    "\n",
    "print(f\"\\nTotal documents added: {len(document_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30ed5be",
   "metadata": {},
   "source": [
    "Test the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "985bee41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      "\n",
      "Based on the context, Chomsky argues that the Gulf War was not a war in the classical sense, as it did not involve direct combat between two sides. Instead, he suggests that it was a covert operation to control Arab oil and crush Arab nationalism, with the primary goal of establishing the US as the world's policeman.\n",
      "\n",
      "Chomsky criticizes the war as unjust and wrong, arguing that the US has no right to control oil prices or administer the future of the Middle East. He also implies that the war is part of a larger pattern of US imperialism, where the country sacrifices its own population to maintain power in other parts of the world.\n",
      "\n",
      "Chomsky's critique of the Gulf War is not primarily focused on the high costs of the war, such as lost lives or economic expenses, but rather on its underlying motivations and consequences. He suggests that liberals who oppose the war for these reasons are missing the point, which is that the war itself is morally reprehensible.\n",
      "\n",
      "Chomsky also notes that the US has little impact on the Soviet system's collapse and that the US response to Saddam Hussein's aggression was a Washington operation with British loyalty, highlighting the US insistence on sole authority in the Middle East.\n",
      "\n",
      "Sources:\n",
      "\n",
      "[1] Aftermath. — (no date)\n",
      "    https://chomsky.info/199110__/\n",
      "[2] The Worst Crime of the 21st Century. — 2023-05-12\n",
      "    https://chomsky.info/20230512-2/\n",
      "[3] Gulf War Pullout. — (no date)\n",
      "    https://chomsky.info/199102__/\n"
     ]
    }
   ],
   "source": [
    "# Build a retriever on top of the existing Chroma vector store\n",
    "# Use a higher k so we can re-rank / filter for more specific matches (e.g. \"Gulf War\").\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 20})\n",
    "\n",
    "system_prompt = \"\"\"You are a helpful assistant that answers questions about Noam Chomsky's\n",
    "articles from `chomsky.info`.\n",
    "\n",
    "You are given context snippets extracted from his articles. Base your answer\n",
    "ONLY on this context. If the answer is not clearly supported by the context,\n",
    "say you don't know and avoid guessing.\n",
    "\n",
    "CRITICAL:\n",
    "- Do NOT invent article titles, dates, or quotes.\n",
    "- Only mention an article title or date if it appears explicitly in the provided context\n",
    "  or if I (the system) separately list it for you.\n",
    "- If the title or date is unclear, say that it is not specified.\n",
    "\n",
    "When relevant, you may summarize the arguments in the context, but do not attribute\n",
    "them to articles that are not explicitly named.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def _build_context(docs, max_chars: int = 6000) -> str:\n",
    "    \"\"\"Concatenate document contents into a single context string.\n",
    "\n",
    "    This avoids depending on newer LangChain chain helpers that may not be\n",
    "    available in your installed version.\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    total = 0\n",
    "    for doc in docs:\n",
    "        text = doc.page_content.strip()\n",
    "        if not text:\n",
    "            continue\n",
    "        if total + len(text) > max_chars:\n",
    "            remaining = max_chars - total\n",
    "            if remaining <= 0:\n",
    "                break\n",
    "            text = text[:remaining]\n",
    "        parts.append(text)\n",
    "        total += len(text)\n",
    "        if total >= max_chars:\n",
    "            break\n",
    "    return \"\\n\\n---\\n\\n\".join(parts)\n",
    "\n",
    "\n",
    "def ask_chomsky(question: str, show_sources: bool = True):\n",
    "    \"\"\"Ask a question about Chomsky's work using a minimal RAG pipeline.\n",
    "\n",
    "    This implementation uses only the base LangChain primitives that are\n",
    "    present even in older versions: it calls the vector-store retriever\n",
    "    directly, manually builds a context string, then calls the chat model.\n",
    "    \"\"\"\n",
    "    # 1. Retrieve relevant chunks\n",
    "    # Some versions expose `.invoke`, older ones use `.get_relevant_documents`.\n",
    "    try:\n",
    "        docs = retriever.invoke(question)\n",
    "    except AttributeError:\n",
    "        docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "    # 1b. Heuristic re-filtering: if the user explicitly mentions \"Gulf War\",\n",
    "    # prefer documents whose title or content also mention \"Gulf War\".\n",
    "    lower_q = question.lower()\n",
    "    if \"gulf war\" in lower_q:\n",
    "        gulf_docs = []\n",
    "        for d in docs:\n",
    "            meta = getattr(d, \"metadata\", {}) or {}\n",
    "            title_text = str(meta.get(\"title\") or meta.get(\"article_title\") or \"\")\n",
    "            blob = (title_text + \"\\n\" + d.page_content).lower()\n",
    "            if \"gulf war\" in blob:\n",
    "                gulf_docs.append(d)\n",
    "        if gulf_docs:\n",
    "            docs = gulf_docs\n",
    "\n",
    "    # 2. Build the context string\n",
    "    context = _build_context(docs)\n",
    "\n",
    "    # 3. Build a single text prompt and ask the model\n",
    "    final_prompt = system_prompt.format(context=context, question=question)\n",
    "    result = model.invoke(final_prompt)\n",
    "\n",
    "    print(\"\\nAnswer:\\n\")\n",
    "    # `result` is a chat message-like object; most versions expose `.content`\n",
    "    print(getattr(result, \"content\", result))\n",
    "\n",
    "    if show_sources:\n",
    "        print(\"\\nSources:\\n\")\n",
    "        seen = set()\n",
    "        uniq_docs = []\n",
    "        for d in docs:\n",
    "            meta = getattr(d, \"metadata\", {}) or {}\n",
    "            key = meta.get(\"source\") or meta.get(\"url\") or id(d)\n",
    "            if key in seen:\n",
    "                continue\n",
    "            seen.add(key)\n",
    "            uniq_docs.append(d)\n",
    "\n",
    "        for i, doc in enumerate(uniq_docs, start=1):\n",
    "            meta = getattr(doc, \"metadata\", {}) or {}\n",
    "            title = meta.get(\"title\") or meta.get(\"article_title\") or \"(untitled)\"\n",
    "            date = meta.get(\"date\") or meta.get(\"article_date\") or \"(no date)\"\n",
    "            url = meta.get(\"source\") or meta.get(\"url\") or \"(no url)\"\n",
    "            print(f\"[{i}] {title} — {date}\\n    {url}\")\n",
    "\n",
    "\n",
    "# Example usage (already active in this cell):\n",
    "ask_chomsky(\"What does Chomsky say about the Gulf War?\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
