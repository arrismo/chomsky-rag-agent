{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9b019e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmoise/Documents/Learning/chomsky-rag-agent/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "import bs4\n",
    "from typing import List\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.agents import create_agent\n",
    "from langchain.tools import tool\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ecbd67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = os.getenv(\"LM_MODEL\")\n",
    "base_url = os.getenv(\"OPENAI_BASE_URL\")\n",
    "\n",
    "model = init_chat_model(\n",
    "    model_name,\n",
    "    model_provider=\"openai\",\n",
    "    base_url=base_url,\n",
    "    api_key=\"lm-studio\"\n",
    "    )\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chomksy-db\", \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8d30b3",
   "metadata": {},
   "source": [
    "Load, Split, Embed, Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5731b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 310 article links\n",
      "EXACT, from main <ul>: 0 article links found:\n",
      "Warning: <ul> found less than 10 articles; falling back to old method.\n",
      "Final: 310 article links.\n",
      "Total characters across all pages: 7579283\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "url = \"https://chomsky.info/articles/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "article_links = []\n",
    "for a in soup.select(\".entry-content a\"):\n",
    "    href = a.get(\"href\", \"\")\n",
    "    # Avoid anchors, archives, and index pages\n",
    "    if href.startswith(\"https://chomsky.info/\") and \"/articles/\" not in href and \"/books/\" not in href and \"#\" not in href and \"pdf\" not in href and href not in article_links:\n",
    "        article_links.append(href)\n",
    "\n",
    "# If the above yields too few, fallback to all single-article permalinks in the list\n",
    "if len(article_links) < 10:\n",
    "    article_links = []\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"]\n",
    "        if href.startswith(\"https://chomsky.info/\") and (\"articles/\" not in href) and (\"#\" not in href) and (\"pdf\" not in href) and href not in article_links:\n",
    "            article_links.append(href)\n",
    "\n",
    "print(f\"Found {len(article_links)} article links\")\n",
    "loader = WebBaseLoader(article_links)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "\n",
    "def get_chomsky_article_links(soup) -> List[str]:\n",
    "    # Find the \"entry-content\" div\n",
    "    entry = soup.find(\"div\", class_=\"entry-content\")\n",
    "    article_links = []\n",
    "    if entry:\n",
    "        # Find the first <ul> inside entry-content\n",
    "        ul = entry.find(\"ul\")\n",
    "        if ul:\n",
    "            for a in ul.find_all(\"a\", href=True):\n",
    "                href = a[\"href\"]\n",
    "                # Exclude PDFs and duplicates\n",
    "                if href.startswith(\"https://chomsky.info/\") and \"pdf\" not in href and href not in article_links:\n",
    "                    article_links.append(href)\n",
    "    return article_links\n",
    "\n",
    "article_links = get_chomsky_article_links(soup)\n",
    "print(f\"EXACT, from main <ul>: {len(article_links)} article links found:\")\n",
    "for link in article_links[:5]:\n",
    "    print(link)\n",
    "\n",
    "if len(article_links) < 10:\n",
    "    print(\"Warning: <ul> found less than 10 articles; falling back to old method.\")\n",
    "    article_links = []\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"]\n",
    "        if href.startswith(\"https://chomsky.info/\") and (\"articles/\" not in href) and (\"#\" not in href) and (\"pdf\" not in href) and href not in article_links:\n",
    "            article_links.append(href)\n",
    "\n",
    "print(f\"Final: {len(article_links)} article links.\")\n",
    "loader = WebBaseLoader(article_links)\n",
    "docs = loader.load()\n",
    "total_chars = sum(len(doc.page_content) for doc in docs)\n",
    "print(f\"Total characters across all pages: {total_chars}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b23d606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 11609 chunks\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# chunk_overlap specifies the number of characters of overlap between consecutive text chunks,\n",
    "# which helps preserve context between splits.\n",
    "# add_start_index, when set to True, includes the start index of the chunk in the split metadata.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200,     \n",
    "    add_start_index=True  \n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "print(f\"Split into {len(all_splits)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1672cc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added batch 1/3 (5000 documents)\n",
      "Added batch 2/3 (5000 documents)\n",
      "Added batch 3/3 (1609 documents)\n",
      "\n",
      "Total documents added: 11609\n"
     ]
    }
   ],
   "source": [
    "# Add documents in batches to avoid exceeding Chroma's max batch size\n",
    "batch_size = 5000  \n",
    "document_ids = []\n",
    "\n",
    "for i in range(0, len(all_splits), batch_size):\n",
    "    batch = all_splits[i:i + batch_size]\n",
    "    batch_ids = vector_store.add_documents(batch)\n",
    "    document_ids.extend(batch_ids)\n",
    "    print(f\"Added batch {i//batch_size + 1}/{(len(all_splits) + batch_size - 1)//batch_size} ({len(batch)} documents)\")\n",
    "\n",
    "print(f\"\\nTotal documents added: {len(document_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e9d825",
   "metadata": {},
   "source": [
    "Construct the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a3c0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve_context(query: str):\n",
    "    \"\"\"Retrieve information to help answer a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs\n",
    "\n",
    "tools = [retrieve_context]\n",
    "prompt = (\n",
    "    \"You are a helpful assistant that can answer questions about Noam Chomsky's articles.\"\n",
    ")\n",
    "\n",
    "agent = create_agent(model, tools, system_prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30ed5be",
   "metadata": {},
   "source": [
    "Test the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1139616a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mWhat did Chomsky say about Jubilee 2000?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m \u001b[43magent\u001b[49m.stream(\n\u001b[32m      4\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: query}]},\n\u001b[32m      5\u001b[39m     stream_mode=\u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m ):\n\u001b[32m      7\u001b[39m     event[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m][-\u001b[32m1\u001b[39m].pretty_print()\n",
      "\u001b[31mNameError\u001b[39m: name 'agent' is not defined"
     ]
    }
   ],
   "source": [
    "query = \"What did Chomsky say about Jubilee 2000?\"\n",
    "\n",
    "for event in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
