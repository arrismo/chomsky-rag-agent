{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df9b019e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmoise/Documents/Learning/chomsky-rag-agent/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "import bs4\n",
    "from typing import List\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.agents import create_agent\n",
    "from langchain.tools import tool\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ecbd67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = os.getenv(\"LM_MODEL\")\n",
    "base_url = os.getenv(\"OPENAI_BASE_URL\")\n",
    "\n",
    "model = init_chat_model(\n",
    "    model_name,\n",
    "    model_provider=\"openai\",\n",
    "    base_url=base_url,\n",
    "    api_key=\"lm-studio\"\n",
    "    )\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chomksy-db\", \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8d30b3",
   "metadata": {},
   "source": [
    "Load, Split, Embed, Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5731b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 328 articles\n",
      "Loaded 328 documents, total characters: 14497624\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "ARTICLES_INDEX_URL = \"https://chomsky.info/articles/\"\n",
    "\n",
    "resp = requests.get(ARTICLES_INDEX_URL)\n",
    "resp.raise_for_status()\n",
    "soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "\n",
    "# Match full dates like \"May 12 2023\", \"March 27, 2023\", month-year like \"October, 1991\", and year only like \"1991\"\n",
    "date_regex = re.compile(\n",
    "    r\"\\b(\"\n",
    "    r\"(?:January|February|March|April|May|June|July|August|September|October|November|December)\"\n",
    "    r\"(?:\\s+\\d{1,2},?)?\\s*,?\\s+\\d{4}\"  # month-day-year OR month-year OR month day, year\n",
    "    r\"|\\d{4}\"  # year only\n",
    "    r\")\\b\"\n",
    ")\n",
    "\n",
    "\n",
    "articles = []\n",
    "for li in soup.select(\"#main_container > ul > li\"):\n",
    "    a = li.find(\"a\")\n",
    "    if not a:\n",
    "        continue\n",
    "    title = a.get_text(strip=True)\n",
    "    href = urljoin(ARTICLES_INDEX_URL, a.get(\"href\", \"\").strip())\n",
    "\n",
    "    # Full text of the <li>\n",
    "    li_text = li.get_text(\" \", strip=True)\n",
    "\n",
    "    # Try to find the full date in the text\n",
    "    m = date_regex.search(li_text)\n",
    "    raw_date = m.group(0) if m else None\n",
    "\n",
    "    # (Optional) normalize to ISO \"YYYY-MM-DD\"\n",
    "    iso_date = None\n",
    "    if raw_date:\n",
    "        cleaned = raw_date.replace(\",\", \"\")\n",
    "        try:\n",
    "            iso_date = datetime.strptime(cleaned, \"%B %d %Y\").date().isoformat()\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    articles.append(\n",
    "        {\n",
    "            \"title\": title,\n",
    "            \"url\": href,\n",
    "            \"raw_date\": raw_date,\n",
    "            \"date\": iso_date or raw_date,  # keep something useful in \"date\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(f\"Found {len(articles)} articles\")\n",
    "\n",
    "# 3. Load full article pages\n",
    "article_urls = [a[\"url\"] for a in articles]\n",
    "loader = WebBaseLoader(article_urls)\n",
    "docs = loader.load()\n",
    "\n",
    "# 4. Attach title and date (and any other metadata) to each Document\n",
    "title_by_url = {a[\"url\"]: a[\"title\"] for a in articles}\n",
    "date_by_url = {a[\"url\"]: a.get(\"date\") for a in articles}\n",
    "\n",
    "for doc in docs:\n",
    "    url = doc.metadata.get(\"source\") or doc.metadata.get(\"url\")\n",
    "    if url in title_by_url:\n",
    "        doc.metadata[\"title\"] = title_by_url[url]\n",
    "    if url in date_by_url and date_by_url[url]:\n",
    "        doc.metadata[\"date\"] = date_by_url[url]\n",
    "    # You can also tag type, etc.\n",
    "    doc.metadata.setdefault(\"type\", \"article\")\n",
    "\n",
    "total_chars = sum(len(doc.page_content) for doc in docs)\n",
    "print(f\"Loaded {len(docs)} documents, total characters: {total_chars}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b23d606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 21595 chunks\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# chunk_overlap specifies the number of characters of overlap between consecutive text chunks,\n",
    "# which helps preserve context between splits.\n",
    "# add_start_index, when set to True, includes the start index of the chunk in the split metadata.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200,     \n",
    "    add_start_index=True  \n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "print(f\"Split into {len(all_splits)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ece2006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('chomsky_chunks.jsonl')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "output_path = Path(\"chomsky_chunks.jsonl\")\n",
    "\n",
    "with output_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for i, doc in enumerate(all_splits):\n",
    "        record = {\n",
    "            \"id\": i,\n",
    "            \"url\": doc.metadata.get(\"source\") or doc.metadata.get(\"url\"),\n",
    "            \"article_title\": doc.metadata.get(\"title\"),\n",
    "            \"article_date\": doc.metadata.get(\"date\"),\n",
    "            \"article_source\": doc.metadata.get(\"source\"),\n",
    "            \"chunk_index\": doc.metadata.get(\"start_index\"),\n",
    "            \"content\": doc.page_content,\n",
    "            \"metadata\": doc.metadata,\n",
    "        }\n",
    "        f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab46126b",
   "metadata": {},
   "source": [
    "### Add documents to Chroma vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1672cc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added batch 1/5 (5000 documents)\n",
      "Added batch 2/5 (5000 documents)\n",
      "Added batch 3/5 (5000 documents)\n",
      "Added batch 4/5 (5000 documents)\n",
      "Added batch 5/5 (1595 documents)\n",
      "\n",
      "Total documents added: 21595\n"
     ]
    }
   ],
   "source": [
    "# Add documents in batches to avoid exceeding Chroma's max batch size\n",
    "batch_size = 5000  \n",
    "document_ids = []\n",
    "\n",
    "for i in range(0, len(all_splits), batch_size):\n",
    "    batch = all_splits[i:i + batch_size]\n",
    "    batch_ids = vector_store.add_documents(batch)\n",
    "    document_ids.extend(batch_ids)\n",
    "    print(f\"Added batch {i//batch_size + 1}/{(len(all_splits) + batch_size - 1)//batch_size} ({len(batch)} documents)\")\n",
    "\n",
    "print(f\"\\nTotal documents added: {len(document_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985bee41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      "\n",
      "According to the context, Chomsky argues that the Soviet Union's propaganda system portrays itself as socialist in order to \"wield the club\" and enforce conformity and obedience to State capitalist institutions. He claims that this portrayal is a powerful ideological weapon used to conceal the destruction of genuine socialist ideals and institutions by the Soviet leadership.\n",
      "\n",
      "Sources:\n",
      "\n",
      "[1] The Soviet Union Versus Socialism. — (no date)\n",
      "    https://chomsky.info/1986____/\n",
      "[2] Symposium on Margaret Boden, Mind as Machine: A History of Cognitive Science. — 2007-10-11\n",
      "    https://chomsky.info/20071011/\n"
     ]
    }
   ],
   "source": [
    "# Build a retriever on top of the existing Chroma vector store\n",
    "# Use a higher k so we can re-rank / filter for more specific matches (e.g. \"Gulf War\").\n",
    "# We use a fairly large k so that we can later filter down to chunks that actually\n",
    "# mention key entities from the question (e.g. \"Soviet Union\").\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 1000})\n",
    "\n",
    "system_prompt = \"\"\"You are a helpful assistant that answers questions about Noam Chomsky's\n",
    "articles from `chomsky.info`.\n",
    "\n",
    "You are given context snippets extracted from his articles. Base your answer\n",
    "ONLY on this context.\n",
    "\n",
    "If the context is only partially relevant, still give the best possible answer\n",
    "you can, clearly indicating any uncertainties. If there is truly no relevant\n",
    "information in the context, then say that you don't know and avoid guessing.\n",
    "\n",
    "CRITICAL:\n",
    "- Do NOT invent article titles, dates, or quotes.\n",
    "- Only mention an article title or date if it appears explicitly in the provided context\n",
    "  or if I (the system) separately list it for you.\n",
    "- If the title or date is unclear, say that it is not specified.\n",
    "\n",
    "When relevant, you may summarize the arguments in the context, but do not attribute\n",
    "them to articles that are not explicitly named.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def _build_context(docs, max_chars: int = 8000) -> str:\n",
    "    \"\"\"Concatenate document contents into a single context string.\n",
    "\n",
    "    This avoids depending on newer LangChain chain helpers that may not be\n",
    "    available in your installed version.\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    total = 0\n",
    "    for doc in docs:\n",
    "        text = (doc.page_content or \"\").strip()\n",
    "        if not text:\n",
    "            continue\n",
    "        if total + len(text) > max_chars:\n",
    "            remaining = max_chars - total\n",
    "            if remaining <= 0:\n",
    "                break\n",
    "            text = text[:remaining]\n",
    "        parts.append(text)\n",
    "        total += len(text)\n",
    "        if total >= max_chars:\n",
    "            break\n",
    "    return \"\\n\\n---\\n\\n\".join(parts)\n",
    "\n",
    "\n",
    "# Simple keyword-based re-ranking so that sources better match the question terms\n",
    "import re\n",
    "\n",
    "_STOPWORDS = {\n",
    "    \"what\",\n",
    "    \"does\",\n",
    "    \"do\",\n",
    "    \"is\",\n",
    "    \"are\",\n",
    "    \"was\",\n",
    "    \"were\",\n",
    "    \"the\",\n",
    "    \"and\",\n",
    "    \"about\",\n",
    "    \"say\",\n",
    "    \"tell\",\n",
    "    \"you\",\n",
    "    \"me\",\n",
    "    \"of\",\n",
    "    \"in\",\n",
    "    \"on\",\n",
    "    \"for\",\n",
    "    \"to\",\n",
    "    \"a\",\n",
    "    \"an\",\n",
    "    # \"union\" is very generic on its own (e.g. trade unions, European Union),\n",
    "    # so treat it as a stopword and instead rely on the full phrase\n",
    "    # \"soviet union\" and on the token \"soviet\" itself.\n",
    "    \"union\",\n",
    "}\n",
    "\n",
    "\n",
    "def _rerank_by_question_keywords(question: str, docs, top_n: int = 15):\n",
    "    \"\"\"Re-rank retrieved docs so that those containing question keywords come first.\n",
    "\n",
    "    This helps ensure that sources actually mention key entities like\n",
    "    \"Soviet Union\" rather than unrelated topics that happened to be\n",
    "    embedding-neighbors.\n",
    "    \"\"\"\n",
    "    q = question.lower()\n",
    "    tokens = re.findall(r\"[a-zA-Z]{4,}\", q)\n",
    "    keywords = [t for t in tokens if t not in _STOPWORDS]\n",
    "\n",
    "    # Simple heuristic expansion: if the question mentions \"soviet\", also\n",
    "    # look for \"ussr\"; you can add similar mappings as needed.\n",
    "    if \"soviet\" in keywords and \"ussr\" not in keywords:\n",
    "        keywords.append(\"ussr\")\n",
    "\n",
    "    # Also treat the full phrase \"soviet union\" specially so we don't\n",
    "    # over-focus on the very generic word \"union\".\n",
    "    phrase_keywords = []\n",
    "    if \"soviet\" in q and \"union\" in q and \"soviet union\" in q:\n",
    "        phrase_keywords.append(\"soviet union\")\n",
    "\n",
    "    if not keywords and not phrase_keywords:\n",
    "        # If no keywords, just return the semantic top N\n",
    "        return docs[:top_n]\n",
    "\n",
    "    def score(doc):\n",
    "        text = (doc.page_content or \"\")\n",
    "        meta = getattr(doc, \"metadata\", {}) or {}\n",
    "        title = str(meta.get(\"title\") or meta.get(\"article_title\") or \"\")\n",
    "        text_l = text.lower()\n",
    "        title_l = title.lower()\n",
    "\n",
    "        # Base score from occurrences in the body\n",
    "        base = sum(text_l.count(k) for k in keywords)\n",
    "        # Strong bonus if the title contains any of the keywords\n",
    "        title_bonus = 0\n",
    "        for k in keywords:\n",
    "            if k in title_l:\n",
    "                title_bonus += 5\n",
    "\n",
    "        # Extra bonus for exact phrase matches (e.g. \"soviet union\")\n",
    "        phrase_bonus = 0\n",
    "        haystack = text_l + \" \" + title_l\n",
    "        for pk in phrase_keywords:\n",
    "            phrase_bonus += haystack.count(pk) * 8\n",
    "\n",
    "        return base + title_bonus + phrase_bonus\n",
    "\n",
    "    scored = [(score(d), d) for d in docs]\n",
    "    if not any(s for s, _ in scored):\n",
    "        # If none of the docs contain any of the keywords, keep original order\n",
    "        return docs[:top_n]\n",
    "\n",
    "    # Keep only documents that have at least one keyword hit, then sort\n",
    "    # by descending score and truncate to top_n.\n",
    "    scored = [(s, d) for s, d in scored if s > 0]\n",
    "    scored.sort(key=lambda x: x[0], reverse=True)\n",
    "    return [d for s, d in scored[:top_n]]\n",
    "\n",
    "\n",
    "def ask_chomsky(question: str, show_sources: bool = True):\n",
    "    \"\"\"Ask a question about Chomsky's work using a minimal RAG pipeline.\n",
    "\n",
    "    This implementation uses only the base LangChain primitives that are\n",
    "    present even in older versions: it calls the vector-store retriever\n",
    "    directly, manually builds a context string, then calls the chat model.\n",
    "    \"\"\"\n",
    "    # 1. Retrieve relevant chunks\n",
    "    # Some versions expose `.invoke`, older ones use `.get_relevant_documents`.\n",
    "    try:\n",
    "        docs = retriever.invoke(question)\n",
    "    except AttributeError:\n",
    "        docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "    # 1b. Re-rank so that documents that actually mention key entities\n",
    "    # (e.g. \"Soviet\", \"Union\", \"USSR\") are preferred.\n",
    "    # Use a modest top_n so that only the most on-topic chunks are kept.\n",
    "    docs = _rerank_by_question_keywords(question, docs, top_n=15)\n",
    "\n",
    "    # 2. Build the context string\n",
    "    # **FIX:** max_chars increased to 16000 in _build_context to accommodate 20 chunks\n",
    "    context = _build_context(docs)\n",
    "\n",
    "    # 2b. Detect whether any of the question keywords actually appear\n",
    "    # in the selected documents. If so, we will *instruct* the model\n",
    "    # not to answer \"I don't know\" and to instead summarize what is there.\n",
    "    # **FIX:** The entire keyword forcing logic has been removed to avoid\n",
    "    # encouraging hallucinations if the context is still weak.\n",
    "    \n",
    "    # 3. Build a single text prompt and ask the model\n",
    "    final_prompt = system_prompt.format(context=context, question=question)\n",
    "    result = model.invoke(final_prompt)\n",
    "\n",
    "    answer_text = getattr(result, \"content\", str(result))\n",
    "\n",
    "    print(\"\\nAnswer:\\n\")\n",
    "    print(answer_text)\n",
    "\n",
    "    # Always show sources so you can see what was retrieved,\n",
    "    # even if the model says it does not know.\n",
    "    if show_sources:\n",
    "        print(\"\\nSources:\\n\")\n",
    "        seen = set()\n",
    "        uniq_docs = []\n",
    "        for d in docs:\n",
    "            meta = getattr(d, \"metadata\", {}) or {}\n",
    "            key = meta.get(\"source\") or meta.get(\"url\") or id(d)\n",
    "            if key in seen:\n",
    "                continue\n",
    "            seen.add(key)\n",
    "            uniq_docs.append(d)\n",
    "\n",
    "        for i, doc in enumerate(uniq_docs, start=1):\n",
    "            meta = getattr(doc, \"metadata\", {}) or {}\n",
    "            title = meta.get(\"title\") or meta.get(\"article_title\") or \"(untitled)\"\n",
    "            date = meta.get(\"date\") or meta.get(\"article_date\") or \"(no date)\"\n",
    "            url = meta.get(\"source\") or meta.get(\"url\") or \"(no url)\"\n",
    "            print(f\"[{i}] {title} — {date}\\n    {url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29f29fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_chomsky(\"What does Chomsky say about the Soviet Union?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
