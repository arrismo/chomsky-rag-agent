{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9b019e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmoise/Documents/Learning/chomsky-rag-agent/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "import bs4\n",
    "from typing import List\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.agents import create_agent\n",
    "from langchain.tools import tool\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ecbd67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = os.getenv(\"LM_MODEL\")\n",
    "base_url = os.getenv(\"OPENAI_BASE_URL\")\n",
    "\n",
    "model = init_chat_model(\n",
    "    model_name,\n",
    "    model_provider=\"openai\",\n",
    "    base_url=base_url,\n",
    "    api_key=\"lm-studio\"\n",
    "    )\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chomksy-db\", \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8d30b3",
   "metadata": {},
   "source": [
    "Load, Split, Embed, Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b5731b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 328 articles\n",
      "Loaded 328 documents, total characters: 14497620\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "ARTICLES_INDEX_URL = \"https://chomsky.info/articles/\"\n",
    "\n",
    "resp = requests.get(ARTICLES_INDEX_URL)\n",
    "resp.raise_for_status()\n",
    "soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "\n",
    "# Match full dates like \"May 12 2023\" or \"March 27, 2023\"\n",
    "date_regex = re.compile(\n",
    "    r\"\\b(\"\n",
    "    r\"January|February|March|April|May|June|July|August|September|October|November|December\"\n",
    "    r\")\\s+\\d{1,2},?\\s+\\d{4}\\b\"\n",
    ")\n",
    "\n",
    "articles = []\n",
    "for li in soup.select(\"#main_container > ul > li\"):\n",
    "    a = li.find(\"a\")\n",
    "    if not a:\n",
    "        continue\n",
    "    title = a.get_text(strip=True)\n",
    "    href = urljoin(ARTICLES_INDEX_URL, a.get(\"href\", \"\").strip())\n",
    "\n",
    "    # Full text of the <li>\n",
    "    li_text = li.get_text(\" \", strip=True)\n",
    "\n",
    "    # Try to find the full date in the text\n",
    "    m = date_regex.search(li_text)\n",
    "    raw_date = m.group(0) if m else None\n",
    "\n",
    "    # (Optional) normalize to ISO \"YYYY-MM-DD\"\n",
    "    iso_date = None\n",
    "    if raw_date:\n",
    "        cleaned = raw_date.replace(\",\", \"\")\n",
    "        try:\n",
    "            iso_date = datetime.strptime(cleaned, \"%B %d %Y\").date().isoformat()\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    articles.append(\n",
    "        {\n",
    "            \"title\": title,\n",
    "            \"url\": href,\n",
    "            \"raw_date\": raw_date,\n",
    "            \"date\": iso_date or raw_date,  # keep something useful in \"date\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(f\"Found {len(articles)} articles\")\n",
    "\n",
    "# 3. Load full article pages\n",
    "article_urls = [a[\"url\"] for a in articles]\n",
    "loader = WebBaseLoader(article_urls)\n",
    "docs = loader.load()\n",
    "\n",
    "# 4. Attach title and date (and any other metadata) to each Document\n",
    "title_by_url = {a[\"url\"]: a[\"title\"] for a in articles}\n",
    "date_by_url = {a[\"url\"]: a.get(\"date\") for a in articles}\n",
    "\n",
    "for doc in docs:\n",
    "    url = doc.metadata.get(\"source\") or doc.metadata.get(\"url\")\n",
    "    if url in title_by_url:\n",
    "        doc.metadata[\"title\"] = title_by_url[url]\n",
    "    if url in date_by_url and date_by_url[url]:\n",
    "        doc.metadata[\"date\"] = date_by_url[url]\n",
    "    # You can also tag type, etc.\n",
    "    doc.metadata.setdefault(\"type\", \"article\")\n",
    "\n",
    "total_chars = sum(len(doc.page_content) for doc in docs)\n",
    "print(f\"Loaded {len(docs)} documents, total characters: {total_chars}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b23d606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 21593 chunks\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# chunk_overlap specifies the number of characters of overlap between consecutive text chunks,\n",
    "# which helps preserve context between splits.\n",
    "# add_start_index, when set to True, includes the start index of the chunk in the split metadata.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200,     \n",
    "    add_start_index=True  \n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "print(f\"Split into {len(all_splits)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ece2006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('chomsky_chunks.jsonl')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "output_path = Path(\"chomsky_chunks.jsonl\")\n",
    "\n",
    "with output_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for i, doc in enumerate(all_splits):\n",
    "        record = {\n",
    "            \"id\": i,\n",
    "            \"url\": doc.metadata.get(\"source\") or doc.metadata.get(\"url\"),\n",
    "            \"article_title\": doc.metadata.get(\"title\"),\n",
    "            \"article_date\": doc.metadata.get(\"date\"),\n",
    "            \"article_source\": doc.metadata.get(\"source\"),\n",
    "            \"chunk_index\": doc.metadata.get(\"start_index\"),\n",
    "            \"content\": doc.page_content,\n",
    "            \"metadata\": doc.metadata,\n",
    "        }\n",
    "        f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1672cc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added batch 1/5 (5000 documents)\n",
      "Added batch 2/5 (5000 documents)\n",
      "Added batch 3/5 (5000 documents)\n",
      "Added batch 4/5 (5000 documents)\n",
      "Added batch 5/5 (1593 documents)\n",
      "\n",
      "Total documents added: 21593\n"
     ]
    }
   ],
   "source": [
    "# Add documents in batches to avoid exceeding Chroma's max batch size\n",
    "batch_size = 5000  \n",
    "document_ids = []\n",
    "\n",
    "for i in range(0, len(all_splits), batch_size):\n",
    "    batch = all_splits[i:i + batch_size]\n",
    "    batch_ids = vector_store.add_documents(batch)\n",
    "    document_ids.extend(batch_ids)\n",
    "    print(f\"Added batch {i//batch_size + 1}/{(len(all_splits) + batch_size - 1)//batch_size} ({len(batch)} documents)\")\n",
    "\n",
    "print(f\"\\nTotal documents added: {len(document_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e9d825",
   "metadata": {},
   "source": [
    "Construct the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30ed5be",
   "metadata": {},
   "source": [
    "Test the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63755111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chomsky said that the Jubilee 2000 call for debt cancellation is \"welcome and merits support, but is open to some qualifications.\" He also mentioned that the debt does not go away, and that someone pays, with the historical record showing that risks are socialized in the system of \"free enterprise capitalism\".\n"
     ]
    }
   ],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "\n",
    "def get_chomsky_context(query: str, k: int = 50):\n",
    "    \"\"\"Retrieve context chunks from the vector store for a user query.\"\"\"\n",
    "    docs = vector_store.similarity_search(query, k=k)\n",
    "    context = \"\\n\\n\".join(\n",
    "        f\"[DOC {i}] {doc.metadata.get('title')} ({doc.metadata.get('date')}):\\n{doc.page_content}\"\n",
    "        for i, doc in enumerate(docs, start=1)\n",
    "    )\n",
    "    return context, docs\n",
    "\n",
    "\n",
    "def answer_with_rag(\n",
    "    question: str,\n",
    "    search_query: str | None = None,\n",
    "    k: int = 4,\n",
    "    use_exact: bool = False,\n",
    "):\n",
    "    \"\"\"Retrieval-augmented generation using the Chroma vector store.\n",
    "\n",
    "    If use_exact=True and search_query is given, pull chunks whose text\n",
    "    contains search_query directly from Chroma via where_document.\n",
    "    Otherwise, fall back to similarity_search.\n",
    "    \"\"\"\n",
    "    if use_exact and search_query:\n",
    "        res = vector_store.get(where_document={\"$contains\": search_query})\n",
    "        texts = res.get(\"documents\", [])\n",
    "        metas = res.get(\"metadatas\", [])\n",
    "        docs = list(zip(texts, metas))\n",
    "        context = \"\\n\\n\".join(\n",
    "            f\"[DOC {i}] {meta.get('title')} ({meta.get('date')}):\\n{text}\"\n",
    "            for i, (text, meta) in enumerate(docs, start=1)\n",
    "        )\n",
    "    else:\n",
    "        effective_query = search_query or question\n",
    "        docs = vector_store.similarity_search(effective_query, k=k)\n",
    "        context = \"\\n\\n\".join(\n",
    "            f\"[DOC {i}] {doc.metadata.get('title')} ({doc.metadata.get('date')}):\\n{doc.page_content}\"\n",
    "            for i, doc in enumerate(docs, start=1)\n",
    "        )\n",
    "\n",
    "    prompt = dedent(\n",
    "        f\"\"\"\n",
    "        You are an assistant that MUST answer ONLY using the context below.\n",
    "        If the answer is not clearly contained in the context, say exactly:\n",
    "        \"I don't know based on the provided context.\"\n",
    "\n",
    "        Do NOT use outside knowledge. Do NOT make up titles, quotes, or sources.\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Answer (using ONLY the context above):\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    response = model.invoke(prompt)\n",
    "    return response, docs\n",
    "\n",
    "\n",
    "def list_articles_mentioning_in_chroma(term: str):\n",
    "    \"\"\"Return unique articles from the Chroma store whose document text contains `term`.\"\"\"\n",
    "    # This uses Chroma's where_document substring filter\n",
    "    res = vector_store.get(where_document={\"$contains\": term})\n",
    "\n",
    "    docs = res.get(\"documents\", [])\n",
    "    metas = res.get(\"metadatas\", [])\n",
    "    ids = res.get(\"ids\", [])\n",
    "\n",
    "    articles = {}\n",
    "    for doc, meta, _id in zip(docs, metas, ids):\n",
    "        source = meta.get(\"source\") or meta.get(\"url\") or _id\n",
    "        if source not in articles:\n",
    "            articles[source] = {\n",
    "                \"url\": source,\n",
    "                \"title\": meta.get(\"title\"),\n",
    "                \"date\": meta.get(\"date\"),\n",
    "                \"chunk_hits\": 0,\n",
    "            }\n",
    "        articles[source][\"chunk_hits\"] += 1\n",
    "\n",
    "    return list(articles.values())\n",
    "\n",
    "\n",
    "resp, docs = answer_with_rag(\n",
    "    question=\"What did Chomsky say about Jubilee 2000?\",\n",
    "    search_query=\"Jubilee 2000\",\n",
    "    use_exact=True,\n",
    ")\n",
    "print(resp.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
