{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df9b019e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ecbd67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmoise/Documents/Learning/chomsky-rag-agent/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "model_name = os.getenv(\"LM_MODEL\")\n",
    "base_url = os.getenv(\"OPENAI_BASE_URL\")\n",
    "\n",
    "model = init_chat_model(\n",
    "    model_name,\n",
    "    model_provider=\"openai\",\n",
    "    base_url=base_url,\n",
    "    api_key=\"lm-studio\"\n",
    "    )\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chomksy-db\", \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8d30b3",
   "metadata": {},
   "source": [
    "Load, Split, Embed, Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b5731b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 310 article links\n",
      "EXACT, from main <ul>: 0 article links found:\n",
      "Warning: <ul> found less than 10 articles; falling back to old method.\n",
      "Final: 310 article links.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Fetch the main page and parse all article links\n",
    "url = \"https://chomsky.info/articles/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find all links that look like article links\n",
    "article_links = []\n",
    "for a in soup.select(\".entry-content a\"):\n",
    "    href = a.get(\"href\", \"\")\n",
    "    # Avoid anchors, archives, and index pages\n",
    "    if href.startswith(\"https://chomsky.info/\") and \"/articles/\" not in href and \"/books/\" not in href and \"#\" not in href and \"pdf\" not in href and href not in article_links:\n",
    "        article_links.append(href)\n",
    "\n",
    "# If the above yields too few, fallback to all single-article permalinks in the list\n",
    "if len(article_links) < 10:\n",
    "    article_links = []\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"]\n",
    "        if href.startswith(\"https://chomsky.info/\") and (\"articles/\" not in href) and (\"#\" not in href) and (\"pdf\" not in href) and href not in article_links:\n",
    "            article_links.append(href)\n",
    "\n",
    "print(f\"Found {len(article_links)} article links\")\n",
    "loader = WebBaseLoader(article_links)\n",
    "\n",
    "docs = loader.load()\n",
    "# The main article links are under <div class=\"entry-content\"> in a <ul> (unordered list)\n",
    "# Each article link is in an <a> tag (with possibly some in blue).\n",
    "# Let's parse links from that specific list, and then load those links.\n",
    "from typing import List\n",
    "\n",
    "def get_chomsky_article_links(soup) -> List[str]:\n",
    "    # Find the \"entry-content\" div\n",
    "    entry = soup.find(\"div\", class_=\"entry-content\")\n",
    "    article_links = []\n",
    "    if entry:\n",
    "        # Find the first <ul> inside entry-content\n",
    "        ul = entry.find(\"ul\")\n",
    "        if ul:\n",
    "            for a in ul.find_all(\"a\", href=True):\n",
    "                href = a[\"href\"]\n",
    "                # Exclude PDFs and duplicates\n",
    "                if href.startswith(\"https://chomsky.info/\") and \"pdf\" not in href and href not in article_links:\n",
    "                    article_links.append(href)\n",
    "    return article_links\n",
    "\n",
    "article_links = get_chomsky_article_links(soup)\n",
    "print(f\"EXACT, from main <ul>: {len(article_links)} article links found:\")\n",
    "for link in article_links[:5]:\n",
    "    print(link)\n",
    "\n",
    "# Optionally, fallback if less than 10 links as before\n",
    "if len(article_links) < 10:\n",
    "    print(\"Warning: <ul> found less than 10 articles; falling back to old method.\")\n",
    "    article_links = []\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"]\n",
    "        if href.startswith(\"https://chomsky.info/\") and (\"articles/\" not in href) and (\"#\" not in href) and (\"pdf\" not in href) and href not in article_links:\n",
    "            article_links.append(href)\n",
    "\n",
    "print(f\"Final: {len(article_links)} article links.\")\n",
    "loader = WebBaseLoader(article_links)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82f7be9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters across all pages: 7579283\n"
     ]
    }
   ],
   "source": [
    "total_chars = sum(len(doc.page_content) for doc in docs)\n",
    "print(f\"Total characters across all pages: {total_chars}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b23d606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 11609 chunks\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# chunk_overlap specifies the number of characters of overlap between consecutive text chunks,\n",
    "# which helps preserve context between splits.\n",
    "# add_start_index, when set to True, includes the start index of the chunk in the split metadata.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200,     \n",
    "    add_start_index=True  \n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "print(f\"Split into {len(all_splits)} chunks\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
